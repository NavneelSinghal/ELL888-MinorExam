\documentclass[a4paper]{article}
\usepackage[english]{babel}
\usepackage[a4paper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{thmtools}
\usepackage{enumitem}
\usepackage[framemethod=TikZ]{mdframed}

\usepackage{xpatch}

\usepackage{boites}
\makeatletter
\xpatchcmd{\endmdframed}
{\aftergroup\endmdf@trivlist\color@endgroup}
{\endmdf@trivlist\color@endgroup\@doendpe}
{}{}
\makeatother

%\usepackage[poster]{tcolorbox}
%\allowdisplaybreaks
%\sloppy

\usepackage[many]{tcolorbox}

\xpatchcmd{\proof}{\itshape}{\bfseries\itshape}{}{}

% to set box separation
\setlength{\fboxsep}{0.8em}
\def\breakboxskip{7pt}
\def\breakboxparindent{0em}

\newenvironment{proof}{\begin{breakbox}\textit{Proof.}}{\hfill$\square$\end{breakbox}}
\newenvironment{ans}{\begin{breakbox}\textit{Answer.}}{\end{breakbox}}
\newenvironment{soln}{\begin{breakbox}\textit{Solution.}}{\end{breakbox}}

% \tcolorboxenvironment{proof}{
%     blanker,
%     before skip=\topsep,
%     after skip=\topsep,
%     borderline={0.4pt}{0.4pt}{black},
%     breakable,
%     left=12pt,
%     right=12pt,
%     top=12pt,
%     bottom=12pt,
% }
%
% \tcolorboxenvironment{ans}{
%     blanker,
%     before skip=\topsep,
%     after skip=\topsep,
%     borderline={0.4pt}{0.4pt}{black},
%     breakable,
%     left=12pt,
%     right=12pt,
% }

\mdfdefinestyle{enclosed}{
    linecolor=black
    ,backgroundcolor=none
    ,apptotikzsetting={\tikzset{mdfbackground/.append style={fill=gray!100,fill opacity=.3}}}
    ,frametitlefont=\sffamily\bfseries\color{black}
    ,splittopskip=.5cm
    ,frametitlebelowskip=.0cm
    ,topline=true
    ,bottomline=true
    ,rightline=true
    ,leftline=true
    ,leftmargin=0.01cm
    ,linewidth=0.02cm
    ,skipabove=0.01cm
    ,innerbottommargin=0.1cm
    ,skipbelow=0.1cm
}

\mdfsetup{%
    middlelinecolor=black,
    middlelinewidth=1pt,
roundcorner=4pt}

\setlength{\parindent}{0pt}

\mdtheorem[style=enclosed]{theorem}{Theorem}
\mdtheorem[style=enclosed]{lemma}{Lemma}[theorem]
\mdtheorem[style=enclosed]{claim}{Claim}[theorem]
\mdtheorem[style=enclosed]{ques}{Question}
\mdtheorem[style=enclosed]{defn}{Definition}
\mdtheorem[style=enclosed]{notn}{Notation}
\mdtheorem[style=enclosed]{obs}{Observation}
\mdtheorem[style=enclosed]{eg}{Example}
\mdtheorem[style=enclosed]{cor}{Corollary}
\mdtheorem[style=enclosed]{note}{Note}

% \let\thetheorem=\relax
% \let\thelemma=\relax
% \let\theclaim=\relax
% \let\theques=\relax
% \let\thedefn=\relax
% \let\thenotn=\relax
% \let\theobs=\relax
% \let\thecor=\relax
% \let\thenote=\relax

% \renewcommand\qedsymbol{$\blacksquare$}
\newcommand{\nl}{\vspace{0.2cm}\\}
\newcommand{\ol}{\overline}
\newcommand{\eps}{\varepsilon}
\newcommand{\mc}{\mathcal}
\newcommand{\mi}{\mathit}
\newcommand{\mf}{\mathbf}
\newcommand{\mb}{\mathbb}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\OPT}{\mathbf{OPT}}
\newcommand{\ALG}{\mathbf{ALG}}
\renewcommand{\L}{\mc{L}}
\newcommand{\changesto}{\vdash}
\newcommand\Vtextvisiblespace[1][.3em]{%
    \mbox{\kern.06em\vrule height.3ex}%
    \vbox{\hrule width#1}%
    \hbox{\vrule height.3ex}
}
\newcommand{\blank}{{\Vtextvisiblespace[0.7em]}}
\newcommand{\leftend}{\triangleright}
\newcommand{\comp}{\overline}

\newcommand{\incfig}[1]{%
    \def\svgwidth{\columnwidth}
    \import{./figures/}{#1.pdf_tex}
}
\pdfsuppresswarningpagegroup=1

\title{\textbf{ELL888 Minor Exam}}
\author{Navneel Singhal}
\date{}

\begin{document}

\maketitle
\tableofcontents

\section{Graph learning setting}

You are given $\mf{X} \in \R^{n \times d}$ whose rows reside on the vertices of an unknown graph.
\begin{align*}
    X = \begin{bmatrix}
        - \mf{x}_1 \in \R^d -\\
        - \mf{x}_2 \in \R^d -\\
            \mid \\
        - \mf{x}_n \in \R^d -\\
    \end{bmatrix}
\end{align*}
Graph learning from data simply means finding an edge-weight matrix $\mf{A} \in \R^{n \times n}$ where its element $w_{ij} = {[A]}_{ij}$ captures the relation between any two pairs of vertices $(i,
j)$, i.e., data points $(\mf{x}_i, \mf{x}_j)$. We learn the graph weights $\mf{w} \in \R^{n(n - 1)/2}$ under certain assumptions like smoothness, probabilistic distribution, nearest
neighbourhood, etc. The focus here is an undirected graph which implies that the graph matrix is symmetric. Now consider the following problems.

\section{Graph learning with missing data}

\subsection{Problem}
\begin{align*}
    X = \begin{bmatrix}
        \mf{x}_1 = [x_{11}, x_{12}, \bullet, x_{14}, \ldots, x_{1d}]\\
        \mf{x}_2 = [x_{21}, \bullet, x_{23}, \bullet, x_{25}, \ldots, x_{1d}]\\
        \mid\\
        \mf{x_n} = [\bullet, x_{n2}, \bullet, x_{n4}, \ldots, x_{nd}]
    \end{bmatrix}
\end{align*}
where some parts of the data are missing at random and $\bullet$ indicates missing data. Explain in detail how we can learn the graph matrix with missing data. Put a descriptive detail.

\subsection{Solution}
I will mainly focus on solving this problem under smoothness constraints.

\subsubsection{Optimization problem formulation}
Firstly, note that a rudimentary imputation scheme is to do the following: for each feature $i \in \{1, \ldots, d\}$, let $D$ be the distribution of the values for the feature that are not
missing. Then, for each $x_i$ such that $x_{id}$ is a missing feature, assign to it a value sampled from $D$. We could also assign it something different --- for instance, the mean of $D$.\nl
However, this is clearly not good enough. Let's look at the formulation of the optimization problem for the weights $\mf{w}$ (with $w(i, j)$ corresponding to the weight of the edge between
$i$ and $j$). It looks something like the following:\\
\begin{align*}
    \min_{w} \sum_{1 \le i < j \le n} w(i, j) ||\mf{x}_i - \mf{x}_j||_2^2\\
    \text{subject to certain constraints on }w
\end{align*}
Note that we are working under the assumption that the signal is smooth. Considering the fact that in graph regularization, such an ``energy'' term is used to penalize the objective function, it
makes sense to consider the above problem as a minimization problem when we vary the missing data as well, so as to ensure that the graph signal is indeed smooth. That is, the problem transforms to the following:

\begin{align*}
    \min_{w, x_{\text{unknown}}} \sum_{1 \le i < j \le n} w(i, j) ||\mf{x}_i - \mf{x}_j||_2^2\\
    \text{subject to certain constraints on }w
\end{align*}

In a sense, alongside the weights $\mf{w}$, we are also learning the missing data under the smoothness assumption.

\subsubsection{Solving the optimization problem}
For solving this optimization problem, we can follow the following iterative method to solve for optimal $w, x_{\text{unknown}}$ given that we have a solver for the original problem as a
black-box (in certain special cases, it is possible to solve this directly rather than relying on the following distribution):

\begin{enumerate}
    \item For each feature $j \in \{1, \ldots, d\}$, compute the multiset $D_j$ of values that appear in $\mf{X}$, and for each $\mf{x}_i$ such that $x_{ij}$ is missing, assign
        to $x_{ij}$ a random sample from $D_j$.
    \item Until a stopping condition is reached, do the following:
        \begin{itemize}
            \item Keeping $x_{\text{unknown}}$ fixed, solve the optimization problem for $\mf{w}$ using the black-box algorithm.
            \item Keeping $\mf{w}$ fixed, solve the quadratic programming problem for $x_{\text{unknown}}$ to get an optimal solution $x^*_{\text{unknown}}$.
        \end{itemize}
    \item Return the learned weights $\mf{w}$.
\end{enumerate}

Here, the first step tries to learn $\mf{w}$ using our prior beliefs about the unknown data, and later on, we try to refine the unknown data by performing some sort of regularization of the
graph signal in order to fit it to a more regular signal.

\subsubsection{Experiments using some standard techniques}
Firstly, a comment on the applicability of this method: note that in the mentioned algorithm, we assumed that we only have access to an oracle that learns weights from a graph (and it is called in
the second part of the loop in our algorithm). Hence, this method is applicable for practically any scenario where we want to learn the weight matrix from data, no matter what the constraints
on $\mf{w}$ that the oracle provides. However, it is not clear if the convergence is guaranteed.\nl
To benchmark how good it is, we shall use the approaches in the following two papers for the aforementioned oracles:

\begin{itemize}
    \item Kalofolias, V. ``How to learn a graph from smooth signals'', AISTATS, 2016.
    \item Dong et al. ``Laplacian Matrix Learning for Smooth Graph Signal Representation''. ICASSP, 2015.
\end{itemize}

\section{Semi-supervised setting}

\subsection{Problem}

\subsection{Solution}


\section{Massive data setting}

\subsection{Problem}

\subsection{Solution}


\section{Heterogeneous data scenario}

\subsection{Problem}

\subsection{Solution}


\end{document}
